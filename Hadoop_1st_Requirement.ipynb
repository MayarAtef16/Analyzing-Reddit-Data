{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayarAtef16/Analyzing-Reddit-Data/blob/main/Hadoop_1st_Requirement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hadoop_Installation**"
      ],
      "metadata": {
        "id": "JAL80wm7SnkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieved from: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb"
      ],
      "metadata": {
        "id": "MtgNkpAPaVHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Installing Java 8"
      ],
      "metadata": {
        "id": "SQwDUe-wSxDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH6-lqgG9SFj",
        "outputId": "44b3b248-a550-4029-fbbd-709a27462ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.17\" 2022-10-18\n",
            "OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu218.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu218.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "#Checking the installed Java version\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# -q, quiet level 2: no output except for errors\n",
        "#> /dev/null on the end of any command where you want to redirect all the stdout into nothingness"
      ],
      "metadata": {
        "id": "hTRtITZK9bDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching java version to use as default (choose option 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD-rcU6a9jBp",
        "outputId": "bcdb380f-04c2-496e-d643-499a3d8561c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching javac version to use as default (choose option 2)\n",
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUvRPoOS9ohu",
        "outputId": "4a65c660-17df-49a4-a547-02f1a8b1858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching jps version to use as default (choose option 2)\n",
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xr9dWu5-pXh",
        "outputId": "71aa7ef2-2f14-4a16-ec9c-2fca03be9745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Java default version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsWnxDTT_YBr",
        "outputId": "7f4b1998-69ab-413b-ab87-fbede25020a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_352\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_352-8u352-ga-1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCg27RB0_etR",
        "outputId": "449f428a-500d-4836-9399-17ae9c7bf010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "PInyq3bR_h8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Installing Secure Shell Server (SSHD)"
      ],
      "metadata": {
        "id": "qB0gjDMxTW9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It is good practice to purge before installation\n",
        "!apt-get purge openssh-server -qq"
      ],
      "metadata": {
        "id": "PyNUJKzb_lm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ],
      "metadata": {
        "id": "RL06IXZm_oDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNLg5Its_rAZ",
        "outputId": "6fbe8958-abc1-4b19-d3b8-2afa565a451e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep Port /etc/ssh/sshd_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI0BqTtA_wKp",
        "outputId": "4232d598-4ad1-4bbb-97c5-a32eb621b6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXtU692o_yVD",
        "outputId": "b378ea67-e5b2-479a-d213-e079bbb58cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:OPvwuFLPkZKXV4p4O+fhMPGWaSmBWi1jSKWJxRWrV3U root@05c4858799c6\n",
            "The key's randomart image is:\n",
            "+---[RSA 2048]----+\n",
            "|   ...+.  . E    |\n",
            "|   o.+ . . .     |\n",
            "|  . + . .        |\n",
            "|   . o =    .    |\n",
            "|    o XoS+ o     |\n",
            "|     ===B+o+     |\n",
            "|    ..o*++O      |\n",
            "|    .  ==B..     |\n",
            "|     .o.o+o      |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the public key\n",
        "!more /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5nTQzQP_1GK",
        "outputId": "4d032367-26b3-4ad5-817b-c08939b8127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC06IXQ3PubeJdtaTnVJ+gLZi+z2Awjf1DjY/MmJ9zg\n",
            "zChEvfM1N+O50IalrkQgR+knaxPR8edDRyc1vQBsUXiY0D28Q1MiTowTM2r1V4bpseTIQx2tXB8m/zOU\n",
            "V4+r0dgS+dxalPRPPgVeuv0s2Gmzm8GIfo8d3Xo1wNKGNDnqWVRAPzrHcbnVTLFn/8xjUO5C3ZZrm94X\n",
            "o3a7A+OmTU9Z/Z0wwu/8wnMJw19FkbmaMOaJmRmEHzGN+vOUn/0SmdMnMPWzemGs7mihz4tWzlx6Jlfd\n",
            "MjIb6/bflqOzahhD+aoGgApYLST0wyJzhwXNP7Infn8zwU25gOirIIGVdWjD root@05c4858799c6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "yKoE9Qg8_3rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcboG-Hl_6bV",
        "outputId": "69d3e604-5d4f-4eae-9d5e-1b2b023b2316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
            " 00:30:23 up 1 min,  0 users,  load average: 2.36, 0.93, 0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "Bjb3fHNzTwc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "rVKdu1mY_9FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file/\n",
        "!rm hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "-BcGk3NAAAXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively"
      ],
      "metadata": {
        "id": "my0lw6ZeBMsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop\n",
        "#we can see various configuration files of hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViM9fwMTBbmD",
        "outputId": "32bb9aa1-4a8f-4b13-8778-a3a2cabe15f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
            "configuration.xsl\t\t  kms-site.xml\n",
            "container-executor.cfg\t\t  log4j.properties\n",
            "core-site.xml\t\t\t  mapred-env.cmd\n",
            "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
            "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
            "hadoop-metrics2.properties\t  mapred-site.xml\n",
            "hadoop-policy.xml\t\t  shellprofile.d\n",
            "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
            "hdfs-site.xml\t\t\t  ssl-server.xml.example\n",
            "httpfs-env.sh\t\t\t  user_ec_policies.xml.template\n",
            "httpfs-log4j.properties\t\t  workers\n",
            "httpfs-signature.secret\t\t  yarn-env.cmd\n",
            "httpfs-site.xml\t\t\t  yarn-env.sh\n",
            "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
            "kms-env.sh\t\t\t  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9lV3HFjBfcU",
        "outputId": "828c4270-f161-43a9-997b-4078387bf5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-openstack,hadoop-azure-datalake,hadoop-aliyun,hadoop-aws,hadoop-azure\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "dtxzXHUtBjFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "id": "ggIGMPWwBoSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop xml files\n",
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTDZ1rrGBqc-",
        "outputId": "b5303f39-a037-449b-f97d-47fe6ed2e2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml file\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yizPSYeNBsm3",
        "outputId": "bc12f7b9-237d-4d56-8323-cd7a39af85e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring mapreduce tools\n",
        "!ls $HADOOP_HOME/share/hadoop/mapreduce/*.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSm5lXAwBu3M",
        "outputId": "2ce74a05-9fa6-4373-95f1-6b790588eaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running Hadoop in Pseudo-distributed mode**"
      ],
      "metadata": {
        "id": "-WArrcH6CY8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "!sed -i '//a\\\n",
        "  \\n\\\n",
        "    fs.defaultFS\\n\\\n",
        "    hdfs://localhost:9000\\n\\\n",
        "  ' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOU97ktFCUuJ",
        "outputId": "286ae20a-5c82-4416-92c0-8c26c873c696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: -e expression #1, char 0: no previous regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "id": "LYAejiS1CcRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fd56fb-48fc-4b67-9b5f-c689913a7ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to hdfs-site.xml file\n",
        "#Since we are running Hadoop in only one machine, a replication factor greater than 1 does not make sense\n",
        "!sed -i '//a\\\n",
        "  \\n\\\n",
        "    dfs.replication\\n\\\n",
        "    1\\n\\\n",
        "  ' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKC6glzlChz9",
        "outputId": "baa427e1-d265-4144-cad9-eaffb25eff27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: -e expression #1, char 0: no previous regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of hdfs-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd_DozoyDohw",
        "outputId": "ed4dc958-558f-4e3e-e3a7-4997fd0d982e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to mapred-site.xml file\n",
        "!sed -i '//a\\\n",
        "  \\n\\\n",
        "    mapreduce.framework.name\\n\\\n",
        "    yarn\\n\\\n",
        "  \\n\\\n",
        "  \\n\\\n",
        "    mapreduce.application.classpath\\n\\\n",
        "    $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*\\n\\\n",
        "  ' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXG8fVfVDq5J",
        "outputId": "7b864d35-ac6d-4946-cc12-d691af44f5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: -e expression #1, char 0: no previous regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of mapred-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3VjqcutDwwj",
        "outputId": "7b79e96c-cc24-4ef8-f4d7-19c37d493648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to yarn-site.xml file\n",
        "!sed -i '//a\\\n",
        "  \\n\\\n",
        "    The hostname of the RM.\\n\\\n",
        "    yarn.resourcemanager.hostname\\n\\\n",
        "    localhost\\n\\\n",
        "  \\n\\\n",
        "  \\n\\\n",
        "    yarn.nodemanager.aux-services\\n\\\n",
        "    mapreduce_shuffle\\n\\\n",
        "  \\n\\\n",
        "  \\n\\\n",
        "    yarn.nodemanager.env-whitelist\\n\\\n",
        "    JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME\\n\\\n",
        "  ' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "awuCkFefD2BK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbba972e-375c-45aa-d870-3658612d485e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: -e expression #1, char 0: no previous regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of yarn-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUQxtKpwYSzo",
        "outputId": "f2cb9fbe-4c1b-47d4-d472-1266026bce95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0h3eZtnYcGX",
        "outputId": "d7ce7fad-c735-45d5-bb5a-616c6c0eeb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2022-11-29 00:31:03,610 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 05c4858799c6/172.28.0.2\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_352\n",
            "************************************************************/\n",
            "2022-11-29 00:31:03,682 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2022-11-29 00:31:03,860 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-da1e8d6c-2d09-4343-b641-72c4b661bb96\n",
            "2022-11-29 00:31:04,677 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2022-11-29 00:31:04,720 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2022-11-29 00:31:04,722 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2022-11-29 00:31:04,723 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2022-11-29 00:31:04,730 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2022-11-29 00:31:04,730 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2022-11-29 00:31:04,730 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2022-11-29 00:31:04,731 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2022-11-29 00:31:04,791 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2022-11-29 00:31:04,822 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2022-11-29 00:31:04,822 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2022-11-29 00:31:04,827 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2022-11-29 00:31:04,828 INFO blockmanagement.BlockManager: The block deletion will start around 2022 Nov 29 00:31:04\n",
            "2022-11-29 00:31:04,830 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2022-11-29 00:31:04,830 INFO util.GSet: VM type       = 64-bit\n",
            "2022-11-29 00:31:04,831 INFO util.GSet: 2.0% max memory 2.8 GB = 57.8 MB\n",
            "2022-11-29 00:31:04,831 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2022-11-29 00:31:04,844 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2022-11-29 00:31:04,845 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2022-11-29 00:31:04,851 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2022-11-29 00:31:04,851 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2022-11-29 00:31:04,852 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2022-11-29 00:31:04,853 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2022-11-29 00:31:04,853 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2022-11-29 00:31:04,877 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2022-11-29 00:31:04,877 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2022-11-29 00:31:04,877 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2022-11-29 00:31:04,878 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2022-11-29 00:31:04,897 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2022-11-29 00:31:04,897 INFO util.GSet: VM type       = 64-bit\n",
            "2022-11-29 00:31:04,897 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2022-11-29 00:31:04,897 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2022-11-29 00:31:04,902 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2022-11-29 00:31:04,903 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2022-11-29 00:31:04,903 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2022-11-29 00:31:04,903 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2022-11-29 00:31:04,912 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2022-11-29 00:31:04,918 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2022-11-29 00:31:04,924 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2022-11-29 00:31:04,924 INFO util.GSet: VM type       = 64-bit\n",
            "2022-11-29 00:31:04,924 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2022-11-29 00:31:04,924 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2022-11-29 00:31:04,938 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2022-11-29 00:31:04,938 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2022-11-29 00:31:04,938 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2022-11-29 00:31:04,943 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2022-11-29 00:31:04,943 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2022-11-29 00:31:04,946 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2022-11-29 00:31:04,946 INFO util.GSet: VM type       = 64-bit\n",
            "2022-11-29 00:31:04,946 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 887.0 KB\n",
            "2022-11-29 00:31:04,946 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2022-11-29 00:31:04,986 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1331338164-172.28.0.2-1669681864975\n",
            "2022-11-29 00:31:05,013 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2022-11-29 00:31:05,056 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2022-11-29 00:31:05,205 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2022-11-29 00:31:05,230 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2022-11-29 00:31:05,275 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2022-11-29 00:31:05,275 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2022-11-29 00:31:05,281 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2022-11-29 00:31:05,281 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 05c4858799c6/172.28.0.2\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop scripts available in sbin directory\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4DrPcATY7UG",
        "outputId": "ed1fa6e0-2257-4b1a-e99f-28e152216fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t start-all.sh\t      stop-balancer.sh\n",
            "FederationStateStore\t start-balancer.sh    stop-dfs.cmd\n",
            "hadoop-daemon.sh\t start-dfs.cmd\t      stop-dfs.sh\n",
            "hadoop-daemons.sh\t start-dfs.sh\t      stop-secure-dns.sh\n",
            "httpfs.sh\t\t start-secure-dns.sh  stop-yarn.cmd\n",
            "kms.sh\t\t\t start-yarn.cmd       stop-yarn.sh\n",
            "mr-jobhistory-daemon.sh  start-yarn.sh\t      workers.sh\n",
            "refresh-namenodes.sh\t stop-all.cmd\t      yarn-daemon.sh\n",
            "start-all.cmd\t\t stop-all.sh\t      yarn-daemons.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "yOZwEF3JZAQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4-n5-7CZCh9",
        "outputId": "8927a44d-ab62-4d9b-a2d0-45a2d6dec4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [05c4858799c6]\n",
            "05c4858799c6: Warning: Permanently added '05c4858799c6,172.28.0.2' (ECDSA) to the list of known hosts.\r\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [05c4858799c6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiUUqLmIZE2H",
        "outputId": "eefb3e3b-c72b-432c-fd5d-46c4d932f5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1580 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9D_xbs3ZIZo",
        "outputId": "a8912922-9a6a-4297-e6f6-d6eec44e03ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: ignoring input and appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_prX8qvPZK2I",
        "outputId": "aced4430-ce85-4187-e6bd-d35064441fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1714 ResourceManager\n",
            "1947 Jps\n",
            "1838 NodeManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH3c8drwZNfG",
        "outputId": "34b45dcb-aefb-4104-e275-18b2d89356bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report: FileSystem file:/// is not an HDFS file system. The fs class is: org.apache.hadoop.fs.LocalFileSystem\n",
            "Usage: hdfs dfsadmin [-report] [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output"
      ],
      "metadata": {
        "id": "KwsyEhtPZSYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The namenode posts the general report on port 9870\n",
        "output.serve_kernel_port_as_window(9870)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nwp0ggfWZWAh",
        "outputId": "134ad51f-1b11-4b61-f5fd-6bfe9166190e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(9870, \"/\", \"https://localhost:9870/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Mount"
      ],
      "metadata": {
        "id": "oDAyC_mBeXis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU6X62PaedK5",
        "outputId": "23a176ec-9c41-40a4-837f-334780fcfc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subreddit Count"
      ],
      "metadata": {
        "id": "WX2_FDy4eQ1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /TopSubreddit"
      ],
      "metadata": {
        "id": "7Nrd5OMfZ1mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' /TopSubreddit"
      ],
      "metadata": {
        "id": "pxU4NRcCZ3Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "i = 0\n",
        "for line in sys.stdin:\n",
        "    if i == 0:\n",
        "        i += 1\n",
        "        continue\n",
        "    if ', ' in line:\n",
        "      line = line.replace(', ',' ')\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[13]\n",
        "        count = 1\n",
        "        print ('%s,%s' % (subreddit, count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S14r8sY9Z5eI",
        "outputId": "25bfa0f7-a930-4311-ed9e-f0d65e504140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "\n",
        "    subreddit, count = line.split(',')\n",
        "    if subreddit in subreddit_count:\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "    else:\n",
        "        subreddit_count[subreddit] = []\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "    total_count = sum(subreddit_count[subreddit])\n",
        "    print(subreddit + \",\" + str(total_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwALib0eZ-Ez",
        "outputId": "9b8cb7fe-f349-4b07-8dd0-b471baf1cd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' | python mapper.py | sort -k1,1 | python reducer.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_VnJY4RaDH9",
        "outputId": "10f04ae0-290d-40ae-8699-f07699aa515d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3Dprinting,1\n",
            "AdviceAnimals,2\n",
            "AgainstGamerGate,1\n",
            "Anarcho_Capitalism,1\n",
            "Android,3\n",
            "anime,3\n",
            "ar15,2\n",
            "AskMen,1\n",
            "AskReddit,29\n",
            "askscience,1\n",
            "AskWomen,1\n",
            "asoiaf,1\n",
            "aspergers,1\n",
            "Assistance,1\n",
            "asstastic,2\n",
            "atheism,2\n",
            "australia,1\n",
            "autism,1\n",
            "Autos,1\n",
            "aww,1\n",
            "BabyBumps,2\n",
            "badkarma,1\n",
            "barstoolsports,1\n",
            "Bass,1\n",
            "batepapo,1\n",
            "battlestations,1\n",
            "beercanada,1\n",
            "beertrade,1\n",
            "Bitcoin,2\n",
            "bjj,1\n",
            "BlackPeopleTwitter,1\n",
            "blackrockshooter,1\n",
            "blog,2\n",
            "BloodWorld,1\n",
            "BMW,1\n",
            "boardgames,1\n",
            "bonnaroo,1\n",
            "BostonBruins,2\n",
            "bravefrontier,2\n",
            "Bravenewbies,1\n",
            "BreedingDittos,1\n",
            "britishproblems,1\n",
            "buffalobills,2\n",
            "buildapc,1\n",
            "Bushcraft,1\n",
            "canada,2\n",
            "CanadaPolitics,1\n",
            "Cardinals,1\n",
            "cars,1\n",
            "casualiama,2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "yFluUOEDaFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' \\\n",
        "  -output /TopSubreddit/subredditCount \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF3IjcLiaJ5A",
        "outputId": "520c543e-9f8c-4f46-cbbc-e624331015c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:32:02,645 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:32:02,768 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:32:02,768 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:32:02,788 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:02,978 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:32:03,004 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:32:03,212 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1656611815_0001\n",
            "2022-11-29 00:32:03,212 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:32:03,418 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:32:03,420 INFO mapreduce.Job: Running job: job_local1656611815_0001\n",
            "2022-11-29 00:32:03,431 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:32:03,433 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:32:03,438 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:03,438 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:03,482 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:32:03,487 INFO mapred.LocalJobRunner: Starting task: attempt_local1656611815_0001_m_000000_0\n",
            "2022-11-29 00:32:03,519 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:03,519 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:03,559 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:03,569 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv:0+159546\n",
            "2022-11-29 00:32:03,584 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:32:03,688 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:32:03,688 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:32:03,688 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:32:03,688 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:32:03,688 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:32:03,692 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:32:03,705 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper.py]\n",
            "2022-11-29 00:32:03,711 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:32:03,712 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:32:03,713 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:32:03,713 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:32:03,714 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:32:03,714 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:32:03,714 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:32:03,714 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:32:03,715 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:32:03,716 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:32:03,716 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:32:03,716 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:32:03,761 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:03,762 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:03,767 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:03,857 INFO streaming.PipeMapRed: Records R/W=455/1\n",
            "2022-11-29 00:32:03,871 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:03,883 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:03,887 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:32:03,887 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:32:03,887 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:32:03,887 INFO mapred.MapTask: bufstart = 0; bufend = 5958; bufvoid = 104857600\n",
            "2022-11-29 00:32:03,887 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26212584(104850336); length = 1813/6553600\n",
            "2022-11-29 00:32:03,908 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:32:03,920 INFO mapred.Task: Task:attempt_local1656611815_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:03,921 INFO mapred.LocalJobRunner: Records R/W=455/1\n",
            "2022-11-29 00:32:03,922 INFO mapred.Task: Task 'attempt_local1656611815_0001_m_000000_0' done.\n",
            "2022-11-29 00:32:03,931 INFO mapred.Task: Final Counters for attempt_local1656611815_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=336269\n",
            "\t\tFILE: Number of bytes written=734175\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=454\n",
            "\t\tMap output bytes=5958\n",
            "\t\tMap output materialized bytes=6872\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=454\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "2022-11-29 00:32:03,931 INFO mapred.LocalJobRunner: Finishing task: attempt_local1656611815_0001_m_000000_0\n",
            "2022-11-29 00:32:03,932 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:32:03,935 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:32:03,935 INFO mapred.LocalJobRunner: Starting task: attempt_local1656611815_0001_r_000000_0\n",
            "2022-11-29 00:32:03,945 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:03,945 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:03,945 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:03,949 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49f600e4\n",
            "2022-11-29 00:32:03,951 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:03,978 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:32:03,987 INFO reduce.EventFetcher: attempt_local1656611815_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:32:04,026 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1656611815_0001_m_000000_0 decomp: 6868 len: 6872 to MEMORY\n",
            "2022-11-29 00:32:04,031 INFO reduce.InMemoryMapOutput: Read 6868 bytes from map-output for attempt_local1656611815_0001_m_000000_0\n",
            "2022-11-29 00:32:04,033 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 6868, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->6868\n",
            "2022-11-29 00:32:04,034 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:32:04,035 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:04,036 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:32:04,044 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:04,044 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6853 bytes\n",
            "2022-11-29 00:32:04,049 INFO reduce.MergeManagerImpl: Merged 1 segments, 6868 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:32:04,049 INFO reduce.MergeManagerImpl: Merging 1 files, 6872 bytes from disk\n",
            "2022-11-29 00:32:04,050 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:32:04,050 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:04,051 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6853 bytes\n",
            "2022-11-29 00:32:04,052 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:04,060 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer.py]\n",
            "2022-11-29 00:32:04,067 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:32:04,069 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:32:04,091 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:04,091 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:04,093 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:04,180 INFO streaming.PipeMapRed: Records R/W=454/1\n",
            "2022-11-29 00:32:04,195 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:04,195 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:04,196 INFO mapred.Task: Task:attempt_local1656611815_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:04,197 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:04,197 INFO mapred.Task: Task attempt_local1656611815_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:32:04,199 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1656611815_0001_r_000000_0' to file:/TopSubreddit/subredditCount\n",
            "2022-11-29 00:32:04,200 INFO mapred.LocalJobRunner: Records R/W=454/1 > reduce\n",
            "2022-11-29 00:32:04,200 INFO mapred.Task: Task 'attempt_local1656611815_0001_r_000000_0' done.\n",
            "2022-11-29 00:32:04,201 INFO mapred.Task: Final Counters for attempt_local1656611815_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=350045\n",
            "\t\tFILE: Number of bytes written=744812\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=275\n",
            "\t\tReduce shuffle bytes=6872\n",
            "\t\tReduce input records=454\n",
            "\t\tReduce output records=275\n",
            "\t\tSpilled Records=454\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=3765\n",
            "2022-11-29 00:32:04,201 INFO mapred.LocalJobRunner: Finishing task: attempt_local1656611815_0001_r_000000_0\n",
            "2022-11-29 00:32:04,201 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:32:04,430 INFO mapreduce.Job: Job job_local1656611815_0001 running in uber mode : false\n",
            "2022-11-29 00:32:04,431 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:32:04,432 INFO mapreduce.Job: Job job_local1656611815_0001 completed successfully\n",
            "2022-11-29 00:32:04,448 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=686314\n",
            "\t\tFILE: Number of bytes written=1478987\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=454\n",
            "\t\tMap output bytes=5958\n",
            "\t\tMap output materialized bytes=6872\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=275\n",
            "\t\tReduce shuffle bytes=6872\n",
            "\t\tReduce input records=454\n",
            "\t\tReduce output records=275\n",
            "\t\tSpilled Records=908\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=3765\n",
            "2022-11-29 00:32:04,449 INFO streaming.StreamJob: Output directory: /TopSubreddit/subredditCount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TopSubreddit/subredditCount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-cVqE82aLiB",
        "outputId": "a37334fb-47b4-40d0-81e4-dd2597964a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:32 /TopSubreddit/subredditCount/_SUCCESS\n",
            "-rw-r--r--   1 root root       3725 2022-11-29 00:32 /TopSubreddit/subredditCount/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TopSubreddit/subredditCount/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II8pjeEkaOCw",
        "outputId": "c1bb08d2-0184-4e5f-84ee-0c689de7cd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3Dprinting,1\t\n",
            "AdviceAnimals,2\t\n",
            "AgainstGamerGate,1\t\n",
            "Anarcho_Capitalism,1\t\n",
            "Android,3\t\n",
            "AskMen,1\t\n",
            "AskReddit,29\t\n",
            "AskWomen,1\t\n",
            "Assistance,1\t\n",
            "Autos,1\t\n",
            "BMW,1\t\n",
            "BabyBumps,2\t\n",
            "Bass,1\t\n",
            "Bitcoin,2\t\n",
            "BlackPeopleTwitter,1\t\n",
            "BloodWorld,1\t\n",
            "BostonBruins,2\t\n",
            "Bravenewbies,1\t\n",
            "BreedingDittos,1\t\n",
            "Bushcraft,1\t\n",
            "CFB,5\t\n",
            "CHIBears,1\t\n",
            "CanadaPolitics,1\t\n",
            "Cardinals,1\t\n",
            "CasualPokemonTrades,1\t\n",
            "Celebs,1\t\n",
            "CoDCompetitive,1\t\n",
            "CodAW,1\t\n",
            "CompetitiveHS,1\t\n",
            "CrazyIdeas,1\t\n",
            "Damnthatsinteresting,1\t\n",
            "DaystromInstitute,1\t\n",
            "DestinyTheGame,4\t\n",
            "DotA2,1\t\n",
            "DotCom,1\t\n",
            "EliteDangerous,3\t\n",
            "EmeraldPS2,1\t\n",
            "EngineeringPorn,1\t\n",
            "EverythingScience,1\t\n",
            "FIFA,1\t\n",
            "FIFACoins,1\t\n",
            "Firearms,1\t\n",
            "Fireteams,1\t\n",
            "Fitness,1\t\n",
            "FixedGearBicycle,1\t\n",
            "FutureWhatIf,1\t\n",
            "Futurology,1\t\n",
            "Games,4\t\n",
            "GirlsCuddlingPuppies,1\t\n",
            "Glitch_in_the_Matrix,1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TopSubreddit/subredditCount/part-00000"
      ],
      "metadata": {
        "id": "pcVNNJDfaQQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55544783-6cf6-47b4-b774-934d998abc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3Dprinting,1\t\n",
            "AdviceAnimals,2\t\n",
            "AgainstGamerGate,1\t\n",
            "Anarcho_Capitalism,1\t\n",
            "Android,3\t\n",
            "AskMen,1\t\n",
            "AskReddit,29\t\n",
            "AskWomen,1\t\n",
            "Assistance,1\t\n",
            "Autos,1\t\n",
            "BMW,1\t\n",
            "BabyBumps,2\t\n",
            "Bass,1\t\n",
            "Bitcoin,2\t\n",
            "BlackPeopleTwitter,1\t\n",
            "BloodWorld,1\t\n",
            "BostonBruins,2\t\n",
            "Bravenewbies,1\t\n",
            "BreedingDittos,1\t\n",
            "Bushcraft,1\t\n",
            "CFB,5\t\n",
            "CHIBears,1\t\n",
            "CanadaPolitics,1\t\n",
            "Cardinals,1\t\n",
            "CasualPokemonTrades,1\t\n",
            "Celebs,1\t\n",
            "CoDCompetitive,1\t\n",
            "CodAW,1\t\n",
            "CompetitiveHS,1\t\n",
            "CrazyIdeas,1\t\n",
            "Damnthatsinteresting,1\t\n",
            "DaystromInstitute,1\t\n",
            "DestinyTheGame,4\t\n",
            "DotA2,1\t\n",
            "DotCom,1\t\n",
            "EliteDangerous,3\t\n",
            "EmeraldPS2,1\t\n",
            "EngineeringPorn,1\t\n",
            "EverythingScience,1\t\n",
            "FIFA,1\t\n",
            "FIFACoins,1\t\n",
            "Firearms,1\t\n",
            "Fireteams,1\t\n",
            "Fitness,1\t\n",
            "FixedGearBicycle,1\t\n",
            "FutureWhatIf,1\t\n",
            "Futurology,1\t\n",
            "Games,4\t\n",
            "GirlsCuddlingPuppies,1\t\n",
            "Glitch_in_the_Matrix,1\t\n",
            "GlobalOffensive,1\t\n",
            "GlobalOffensiveTrade,2\t\n",
            "GoneWildPlus,1\t\n",
            "GrandTheftAutoV,1\t\n",
            "HaloPlayers,1\t\n",
            "HannibalTV,1\t\n",
            "HeroRP,1\t\n",
            "HistoricalPowers,3\t\n",
            "Homebrewing,1\t\n",
            "Insurance,1\t\n",
            "InsurgenceBattles,1\t\n",
            "Judaism,1\t\n",
            "KillLaKill,1\t\n",
            "Knoxville,1\t\n",
            "KotakuInAction,1\t\n",
            "LSD,1\t\n",
            "LiverpoolFC,1\t\n",
            "MLPLounge,5\t\n",
            "MLS,1\t\n",
            "MMA,2\t\n",
            "MakeupAddiction,1\t\n",
            "Metroid,1\t\n",
            "Military,1\t\n",
            "NSFW_GIF,1\t\n",
            "Naruto,1\t\n",
            "NoFap,1\t\n",
            "OkCupid,1\t\n",
            "OldSchoolCool,1\t\n",
            "Omnipotent_League,3\t\n",
            "PKA,1\t\n",
            "Padres,1\t\n",
            "PercyJacksonRP,2\t\n",
            "PictureGame,1\t\n",
            "Planetside,2\t\n",
            "PokemonPlaza,2\t\n",
            "Portland,1\t\n",
            "ProRevenge,1\t\n",
            "PuzzleAndDragons,2\t\n",
            "RWBY,1\t\n",
            "RagenChastain,1\t\n",
            "RandomActsOfGaming,1\t\n",
            "RandomActsOfSTC,1\t\n",
            "Random_Acts_Of_Amazon,2\t\n",
            "Rateme,1\t\n",
            "RealEstate,1\t\n",
            "RealGirls,1\t\n",
            "Redskins,1\t\n",
            "Reformed,2\t\n",
            "SNSD,1\t\n",
            "SVExchange,1\t\n",
            "Scrubs,1\t\n",
            "Seahawks,1\t\n",
            "Seattle,1\t\n",
            "Showerthoughts,1\t\n",
            "Smite,3\t\n",
            "Sneakers,2\t\n",
            "SquaredCircle,1\t\n",
            "Stacked,1\t\n",
            "StarWars,1\t\n",
            "TeamCanada,1\t\n",
            "TheLastAirbender,1\t\n",
            "TheWayWeWere,1\t\n",
            "TopGear,1\t\n",
            "TrollXChromosomes,2\t\n",
            "Trucks,1\t\n",
            "TumblrInAction,2\t\n",
            "TwoXChromosomes,1\t\n",
            "WTF,5\t\n",
            "Warframe,1\t\n",
            "Waxpen,1\t\n",
            "Wet_Shavers,1\t\n",
            "Wishlist,5\t\n",
            "Yogscast,1\t\n",
            "anime,3\t\n",
            "ar15,2\t\n",
            "askscience,1\t\n",
            "asoiaf,1\t\n",
            "aspergers,1\t\n",
            "asstastic,2\t\n",
            "atheism,2\t\n",
            "australia,1\t\n",
            "autism,1\t\n",
            "aww,1\t\n",
            "badkarma,1\t\n",
            "barstoolsports,1\t\n",
            "batepapo,1\t\n",
            "battlestations,1\t\n",
            "beercanada,1\t\n",
            "beertrade,1\t\n",
            "bjj,1\t\n",
            "blackrockshooter,1\t\n",
            "blog,2\t\n",
            "boardgames,1\t\n",
            "bonnaroo,1\t\n",
            "bravefrontier,2\t\n",
            "britishproblems,1\t\n",
            "buffalobills,2\t\n",
            "buildapc,1\t\n",
            "canada,2\t\n",
            "cars,1\t\n",
            "casualiama,2\t\n",
            "changemyview,1\t\n",
            "childfree,1\t\n",
            "comicswap,1\t\n",
            "curvy,1\t\n",
            "darksouls,1\t\n",
            "dayz,1\t\n",
            "dbz,1\t\n",
            "depression,1\t\n",
            "devils,2\t\n",
            "dogecoin,1\t\n",
            "drums,1\t\n",
            "drunk,1\t\n",
            "drunkvapes,1\t\n",
            "electronic_cigarette,2\t\n",
            "elliottsmith,1\t\n",
            "exjw,1\t\n",
            "exmormon,1\t\n",
            "explainlikeimfive,4\t\n",
            "falcons,1\t\n",
            "ffxiv,1\t\n",
            "fireemblem,1\t\n",
            "firstworldproblems,1\t\n",
            "food,1\t\n",
            "freedonuts,1\t\n",
            "funny,13\t\n",
            "gamecollecting,1\t\n",
            "gaming,2\t\n",
            "genetics,1\t\n",
            "gifs,2\t\n",
            "giftcardexchange,1\t\n",
            "guitarpedals,1\t\n",
            "guns,1\t\n",
            "gwcumsluts,1\t\n",
            "headphones,1\t\n",
            "hearthstone,1\t\n",
            "hiphopheads,3\t\n",
            "hockey,9\t\n",
            "hometheater,1\t\n",
            "iphone,1\t\n",
            "knives,2\t\n",
            "korrasami,1\t\n",
            "kpop,1\t\n",
            "leagueoflegends,6\t\n",
            "learnprogramming,1\t\n",
            "learnpython,1\t\n",
            "linux,1\t\n",
            "lotr,1\t\n",
            "magicTCG,1\t\n",
            "marvelstudios,1\t\n",
            "mercedes,1\t\n",
            "mildlyinfuriating,1\t\n",
            "milwaukee,1\t\n",
            "mistyfront,1\t\n",
            "movies,1\t\n",
            "mtgfinance,1\t\n",
            "nba,4\t\n",
            "needadvice,1\t\n",
            "neopets,1\t\n",
            "nerdcubed,1\t\n",
            "newhampshire,1\t\n",
            "news,5\t\n",
            "nfl,6\t\n",
            "notinteresting,1\t\n",
            "offmychest,1\t\n",
            "opiates,1\t\n",
            "pcmasterrace,14\t\n",
            "perktv,1\t\n",
            "personalfinance,1\t\n",
            "pics,10\t\n",
            "playrust,1\t\n",
            "pokemon,1\t\n",
            "pokemontrades,1\t\n",
            "politics,3\t\n",
            "powerrangers,1\t\n",
            "predaddit,1\t\n",
            "projectcar,1\t\n",
            "pussypassdenied,1\t\n",
            "rage,1\t\n",
            "randomactsofcsgo,1\t\n",
            "randomsuperpowers,2\t\n",
            "relationships,1\t\n",
            "roosterteeth,2\t\n",
            "rwbyRP,2\t\n",
            "sausagetalk,1\t\n",
            "serialpodcast,1\t\n",
            "sex,1\t\n",
            "singapore,1\t\n",
            "smashbros,4\t\n",
            "soccer,4\t\n",
            "sports,1\t\n",
            "ssbbw,1\t\n",
            "starcraft,1\t\n",
            "startrek,1\t\n",
            "steroids,1\t\n",
            "subaru,1\t\n",
            "subredditreports,3\t\n",
            "summonerschool,1\t\n",
            "surfing,1\t\n",
            "swtor,1\t\n",
            "tarot,1\t\n",
            "techsupport,1\t\n",
            "teenagers,1\t\n",
            "tf2,3\t\n",
            "thelastofus,1\t\n",
            "thelastofusfactions,1\t\n",
            "tifu,2\t\n",
            "tmobile,1\t\n",
            "todayilearned,4\t\n",
            "touhou,1\t\n",
            "transpassing,1\t\n",
            "trashy,1\t\n",
            "travel,1\t\n",
            "trees,1\t\n",
            "truegaming,1\t\n",
            "twinks,1\t\n",
            "vancouver,1\t\n",
            "vexillology,1\t\n",
            "videos,5\t\n",
            "weddingplanning,1\t\n",
            "weeabootales,1\t\n",
            "wicked_edge,1\t\n",
            "worldnews,2\t\n",
            "wow,2\t\n",
            "xboxone,1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /TopSubreddit/subredditCount/part-00000 >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_count.txt'"
      ],
      "metadata": {
        "id": "8LNgD1aZot9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapperReducer **\"\"Subreddit Top\"\"**"
      ],
      "metadata": {
        "id": "5XoYOA_Qn2TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "#!$HADOOP_HOME/bin/hdfs dfs -mkdir /TopSubreddit"
      ],
      "metadata": {
        "id": "kNsvaragn2TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_count.txt' /TopSubreddit"
      ],
      "metadata": {
        "id": "4Ue69FDBn2Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper2.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[0]\n",
        "        count = line[1]\n",
        "        print ('%s,%s' % (subreddit, count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fb0f34-5237-4f8b-d617-fbad5b173acd",
        "id": "-nXB8kkxn2Tb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile combiner2.py\n",
        "\n",
        "#!/usr/bin/python\n",
        "#combiner.py\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, count = line.split(',')\n",
        "    if subreddit in subreddit_count:\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "    else:\n",
        "        subreddit_count[subreddit] = []\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "#Reducer\n",
        "\n",
        "subreddit_count = dict(sorted(subreddit_count.items(), key = lambda x: x[1], reverse = True))\n",
        "#print(total_count)\n",
        "#total_count = sum(subreddit_count[subreddit])\n",
        "for subreddit in subreddit_count:\n",
        "    print(subreddit+\",\" + str(subreddit_count[subreddit][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nfCUOzyoHoj",
        "outputId": "de79a14b-5866-4a82-a98e-875fe50fe317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing combiner2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer2.py\n",
        "#!/usr/bin/python\n",
        "#combiner.py\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, count = line.split(',')\n",
        "    if subreddit in subreddit_count:\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "    else:\n",
        "        subreddit_count[subreddit] = []\n",
        "        subreddit_count[subreddit].append(int(count))\n",
        "i = 0\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "    print(subreddit + \"\\t\" + str(subreddit_count[subreddit][0]))\n",
        "    i += 1\n",
        "    if i == 10:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3018fea9-2772-403c-f679-79931bd10fd5",
        "id": "6vYJ05OUn2Tc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_count.txt' | python mapper2.py | sort -k1,1 | python reducer2.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8ca2ae-2d7f-4b10-8950-febe803eefbf",
        "id": "GxNSCyYPn2Td"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3Dprinting\t1\n",
            "AdviceAnimals\t2\n",
            "AgainstGamerGate\t1\n",
            "Anarcho_Capitalism\t1\n",
            "Android\t3\n",
            "anime\t3\n",
            "ar15\t2\n",
            "AskMen\t1\n",
            "AskReddit\t29\n",
            "askscience\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper2.py /content/reducer2.py /content/combiner2.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "46nujAo4n2Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_count.txt' \\\n",
        "  -combiner \"python /content/combiner2.py\" \\\n",
        "  -output /TopSubreddit/subredditTopCount2 \\\n",
        "  -mapper \"python /content/mapper2.py\" \\\n",
        "  -reducer \"python /content/reducer2.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552e9a21-15c6-4e9f-e950-aa178a43c2d8",
        "id": "woHlW5Can2Te"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:32:14,688 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:32:14,820 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:32:14,820 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:32:14,839 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:15,074 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:32:15,109 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:32:15,345 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2136454449_0001\n",
            "2022-11-29 00:32:15,345 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:32:15,516 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:32:15,518 INFO mapreduce.Job: Running job: job_local2136454449_0001\n",
            "2022-11-29 00:32:15,526 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:32:15,527 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:32:15,532 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:15,532 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:15,571 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:32:15,575 INFO mapred.LocalJobRunner: Starting task: attempt_local2136454449_0001_m_000000_0\n",
            "2022-11-29 00:32:15,607 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:15,608 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:15,634 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:15,643 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_count.txt:0+18632\n",
            "2022-11-29 00:32:15,665 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:32:15,752 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:32:15,752 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:32:15,752 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:32:15,752 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:32:15,752 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:32:15,757 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:32:15,770 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper2.py]\n",
            "2022-11-29 00:32:15,780 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:32:15,780 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:32:15,781 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:32:15,781 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:32:15,785 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:32:15,785 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:32:15,786 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:32:15,786 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:32:15,787 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:32:15,787 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:32:15,788 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:32:15,788 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:32:15,828 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:15,828 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:15,832 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:15,852 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:15,918 INFO streaming.PipeMapRed: Records R/W=1375/1\n",
            "2022-11-29 00:32:15,942 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:15,943 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:15,947 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:32:15,947 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:32:15,947 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:32:15,947 INFO mapred.MapTask: bufstart = 0; bufend = 18632; bufvoid = 104857600\n",
            "2022-11-29 00:32:15,948 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26208900(104835600); length = 5497/6553600\n",
            "2022-11-29 00:32:15,999 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/combiner2.py]\n",
            "2022-11-29 00:32:16,004 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
            "2022-11-29 00:32:16,026 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,026 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,027 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,045 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,140 INFO streaming.PipeMapRed: Records R/W=1375/1\n",
            "2022-11-29 00:32:16,147 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:16,147 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:16,148 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:32:16,162 INFO mapred.Task: Task:attempt_local2136454449_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:16,164 INFO mapred.LocalJobRunner: Records R/W=1375/1\n",
            "2022-11-29 00:32:16,165 INFO mapred.Task: Task 'attempt_local2136454449_0001_m_000000_0' done.\n",
            "2022-11-29 00:32:16,175 INFO mapred.Task: Final Counters for attempt_local2136454449_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=195397\n",
            "\t\tFILE: Number of bytes written=732621\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1375\n",
            "\t\tMap output records=1375\n",
            "\t\tMap output bytes=18632\n",
            "\t\tMap output materialized bytes=4306\n",
            "\t\tInput split bytes=176\n",
            "\t\tCombine input records=1375\n",
            "\t\tCombine output records=276\n",
            "\t\tSpilled Records=276\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18632\n",
            "2022-11-29 00:32:16,175 INFO mapred.LocalJobRunner: Finishing task: attempt_local2136454449_0001_m_000000_0\n",
            "2022-11-29 00:32:16,175 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:32:16,179 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:32:16,180 INFO mapred.LocalJobRunner: Starting task: attempt_local2136454449_0001_r_000000_0\n",
            "2022-11-29 00:32:16,188 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:16,188 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:16,188 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:16,195 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5d64dc2d\n",
            "2022-11-29 00:32:16,198 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:16,219 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:32:16,232 INFO reduce.EventFetcher: attempt_local2136454449_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:32:16,271 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2136454449_0001_m_000000_0 decomp: 4302 len: 4306 to MEMORY\n",
            "2022-11-29 00:32:16,278 INFO reduce.InMemoryMapOutput: Read 4302 bytes from map-output for attempt_local2136454449_0001_m_000000_0\n",
            "2022-11-29 00:32:16,279 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4302\n",
            "2022-11-29 00:32:16,281 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:32:16,282 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:16,282 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:32:16,294 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:16,294 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4287 bytes\n",
            "2022-11-29 00:32:16,305 INFO reduce.MergeManagerImpl: Merged 1 segments, 4302 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:32:16,305 INFO reduce.MergeManagerImpl: Merging 1 files, 4306 bytes from disk\n",
            "2022-11-29 00:32:16,308 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:32:16,308 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:16,310 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4287 bytes\n",
            "2022-11-29 00:32:16,310 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:16,319 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer2.py]\n",
            "2022-11-29 00:32:16,323 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:32:16,323 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:32:16,355 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,355 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,356 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:16,443 INFO streaming.PipeMapRed: Records R/W=276/1\n",
            "2022-11-29 00:32:16,447 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:16,447 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:16,448 INFO mapred.Task: Task:attempt_local2136454449_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:16,449 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:16,449 INFO mapred.Task: Task attempt_local2136454449_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:32:16,451 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2136454449_0001_r_000000_0' to file:/TopSubreddit/subredditTopCount2\n",
            "2022-11-29 00:32:16,457 INFO mapred.LocalJobRunner: Records R/W=276/1 > reduce\n",
            "2022-11-29 00:32:16,457 INFO mapred.Task: Task 'attempt_local2136454449_0001_r_000000_0' done.\n",
            "2022-11-29 00:32:16,458 INFO mapred.Task: Final Counters for attempt_local2136454449_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=204041\n",
            "\t\tFILE: Number of bytes written=737042\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=276\n",
            "\t\tReduce shuffle bytes=4306\n",
            "\t\tReduce input records=276\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=276\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=115\n",
            "2022-11-29 00:32:16,458 INFO mapred.LocalJobRunner: Finishing task: attempt_local2136454449_0001_r_000000_0\n",
            "2022-11-29 00:32:16,458 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:32:16,524 INFO mapreduce.Job: Job job_local2136454449_0001 running in uber mode : false\n",
            "2022-11-29 00:32:16,525 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:32:16,526 INFO mapreduce.Job: Job job_local2136454449_0001 completed successfully\n",
            "2022-11-29 00:32:16,540 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=399438\n",
            "\t\tFILE: Number of bytes written=1469663\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1375\n",
            "\t\tMap output records=1375\n",
            "\t\tMap output bytes=18632\n",
            "\t\tMap output materialized bytes=4306\n",
            "\t\tInput split bytes=176\n",
            "\t\tCombine input records=1375\n",
            "\t\tCombine output records=276\n",
            "\t\tReduce input groups=276\n",
            "\t\tReduce shuffle bytes=4306\n",
            "\t\tReduce input records=276\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=552\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18632\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=115\n",
            "2022-11-29 00:32:16,540 INFO streaming.StreamJob: Output directory: /TopSubreddit/subredditTopCount2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TopSubreddit/subredditTopCount2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe0dc05-16e3-4258-dd0f-294df0104a3b",
        "id": "KPaSqk_rn2Tf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:32 /TopSubreddit/subredditTopCount2/_SUCCESS\n",
            "-rw-r--r--   1 root root        103 2022-11-29 00:32 /TopSubreddit/subredditTopCount2/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TopSubreddit/subredditTopCount2/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6b9258-3aa4-42f9-ba0a-6522206347b5",
        "id": "mTQjh0axn2Tg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit\t29\n",
            "pcmasterrace\t14\n",
            "funny\t13\n",
            "pics\t10\n",
            "hockey\t9\n",
            "leagueoflegends\t6\n",
            "nfl\t6\n",
            "CFB\t5\n",
            "MLPLounge\t5\n",
            "WTF\t5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -text /TopSubreddit/subredditTopCount2/part-00000  >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/1st Requirements/Top Subreddit/subreddit_Top_count.txt'"
      ],
      "metadata": {
        "id": "VDkawQ9lqub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top linked_iD 1"
      ],
      "metadata": {
        "id": "h4ARu5e-csmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /Toplink_id"
      ],
      "metadata": {
        "id": "6ATDjuZ1dSLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' /Toplink_id"
      ],
      "metadata": {
        "id": "ub0HybH3dSLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper3.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "i=0\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "TopSubreddit = ['AskReddit', 'pcmasterrace', 'funny', 'pics', 'hockey', 'nfl', 'Games', 'news', 'leagueoflegends', 'CFB']\n",
        "# reading entire line from STDIN (standard input)\n",
        "\n",
        "for line in sys.stdin:\n",
        "    if i == 0:\n",
        "        i += 1\n",
        "        continue\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[13]\n",
        "        link_id = line [3]\n",
        "        count = 1\n",
        "        if subreddit in TopSubreddit:\n",
        "           print (subreddit+','+link_id+','+str(count))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a957f30b-f8de-4c0a-e283-8eb1edee3956",
        "id": "l-nSF6TRdSLd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer3.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, link_id, count = line.split(',')\n",
        "    if (subreddit,link_id) in subreddit_count:\n",
        "        subreddit_count[(subreddit,link_id)].append(int(count))\n",
        "    else:\n",
        "        subreddit_count[(subreddit,link_id)] = []\n",
        "        subreddit_count[(subreddit,link_id)].append(int(count))\n",
        "\n",
        "\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "    total_count = sum(subreddit_count[subreddit])\n",
        "    print(str(subreddit[0])+','+str(subreddit[1]) + \",\" + str(total_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f386b53a-66e2-4fb2-f8ac-d0639f0bc660",
        "id": "JbKZeWzydSLd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' | python mapper3.py | sort -k1,1 | python reducer3.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2758e3ad-81aa-41c5-a495-b8ac6a88572a",
        "id": "KwKGwfGFdSLe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,t3_2pidx3,1\n",
            "AskReddit,t3_2qwm98,2\n",
            "AskReddit,t3_2qx5z4,1\n",
            "AskReddit,t3_2qx9vw,2\n",
            "AskReddit,t3_2qxfa5,1\n",
            "AskReddit,t3_2qxogl,1\n",
            "AskReddit,t3_2qxxsz,1\n",
            "AskReddit,t3_2qy2qk,2\n",
            "AskReddit,t3_2qy36l,1\n",
            "AskReddit,t3_2qy8r4,6\n",
            "AskReddit,t3_2qyig3,1\n",
            "AskReddit,t3_2qykcw,2\n",
            "AskReddit,t3_2qykg1,1\n",
            "AskReddit,t3_2qykih,1\n",
            "AskReddit,t3_2qylfy,1\n",
            "AskReddit,t3_2qynwn,1\n",
            "AskReddit,t3_2qyobl,1\n",
            "AskReddit,t3_2qyqs9,1\n",
            "AskReddit,t3_2qyrgt,1\n",
            "AskReddit,t3_2qyrje,1\n",
            "CFB,t3_2qy42q,1\n",
            "CFB,t3_2qy94r,3\n",
            "CFB,t3_2qyppp,1\n",
            "funny,t3_2qx73h,1\n",
            "funny,t3_2qxc5y,1\n",
            "funny,t3_2qxkcx,1\n",
            "funny,t3_2qxubc,1\n",
            "funny,t3_2qy0ft,5\n",
            "funny,t3_2qy7at,1\n",
            "funny,t3_2qyoc4,1\n",
            "funny,t3_2qyqyr,1\n",
            "funny,t3_2qyrvt,1\n",
            "Games,t3_2qwmyo,1\n",
            "Games,t3_2qxl90,2\n",
            "Games,t3_2qy45k,1\n",
            "hockey,t3_2qxi9c,1\n",
            "hockey,t3_2qxjzp,1\n",
            "hockey,t3_2qy54g,1\n",
            "hockey,t3_2qyim8,1\n",
            "hockey,t3_2qyjqv,2\n",
            "hockey,t3_2qyktv,1\n",
            "hockey,t3_2qyp3l,1\n",
            "hockey,t3_2qypbg,1\n",
            "leagueoflegends,t3_2qwdix,1\n",
            "leagueoflegends,t3_2qxntu,2\n",
            "leagueoflegends,t3_2qxzjm,1\n",
            "leagueoflegends,t3_2qy50m,1\n",
            "leagueoflegends,t3_2qyq44,1\n",
            "news,t3_2qwzdl,1\n",
            "news,t3_2qx8kh,4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper3.py /content/reducer3.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "elNRXlIIdSLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' \\\n",
        "  -output /Toplink_id/Toplink_idout_11 \\\n",
        "  -mapper \"python /content/mapper3.py\" \\\n",
        "  -reducer \"python /content/reducer3.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fe891e-6b24-4d13-c4f6-24e019001426",
        "id": "a5BIUN2mdSLe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:32:28,008 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:32:28,111 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:32:28,111 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:32:28,141 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:28,328 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:32:28,352 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:32:28,586 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1843049360_0001\n",
            "2022-11-29 00:32:28,586 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:32:28,781 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:32:28,783 INFO mapreduce.Job: Running job: job_local1843049360_0001\n",
            "2022-11-29 00:32:28,790 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:32:28,792 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:32:28,802 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:28,802 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:28,844 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:32:28,848 INFO mapred.LocalJobRunner: Starting task: attempt_local1843049360_0001_m_000000_0\n",
            "2022-11-29 00:32:28,884 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:28,884 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:28,911 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:28,921 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv:0+159546\n",
            "2022-11-29 00:32:28,939 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:32:29,009 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:32:29,009 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:32:29,009 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:32:29,009 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:32:29,009 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:32:29,012 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:32:29,020 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper3.py]\n",
            "2022-11-29 00:32:29,027 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:32:29,027 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:32:29,028 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:32:29,028 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:32:29,029 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:32:29,029 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:32:29,029 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:32:29,030 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:32:29,030 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:32:29,031 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:32:29,031 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:32:29,032 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:32:29,065 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,066 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,070 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,163 INFO streaming.PipeMapRed: Records R/W=455/1\n",
            "2022-11-29 00:32:29,173 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:29,174 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:29,178 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:32:29,178 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:32:29,178 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:32:29,178 INFO mapred.MapTask: bufstart = 0; bufend = 2165; bufvoid = 104857600\n",
            "2022-11-29 00:32:29,178 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213996(104855984); length = 401/6553600\n",
            "2022-11-29 00:32:29,204 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:32:29,217 INFO mapred.Task: Task:attempt_local1843049360_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:29,219 INFO mapred.LocalJobRunner: Records R/W=455/1\n",
            "2022-11-29 00:32:29,219 INFO mapred.Task: Task 'attempt_local1843049360_0001_m_000000_0' done.\n",
            "2022-11-29 00:32:29,229 INFO mapred.Task: Final Counters for attempt_local1843049360_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=336269\n",
            "\t\tFILE: Number of bytes written=729680\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2165\n",
            "\t\tMap output materialized bytes=2373\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=101\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "2022-11-29 00:32:29,230 INFO mapred.LocalJobRunner: Finishing task: attempt_local1843049360_0001_m_000000_0\n",
            "2022-11-29 00:32:29,231 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:32:29,234 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:32:29,235 INFO mapred.LocalJobRunner: Starting task: attempt_local1843049360_0001_r_000000_0\n",
            "2022-11-29 00:32:29,243 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:29,243 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:29,244 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:29,247 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1df73a1e\n",
            "2022-11-29 00:32:29,249 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:29,273 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:32:29,285 INFO reduce.EventFetcher: attempt_local1843049360_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:32:29,313 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1843049360_0001_m_000000_0 decomp: 2369 len: 2373 to MEMORY\n",
            "2022-11-29 00:32:29,318 INFO reduce.InMemoryMapOutput: Read 2369 bytes from map-output for attempt_local1843049360_0001_m_000000_0\n",
            "2022-11-29 00:32:29,319 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2369, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2369\n",
            "2022-11-29 00:32:29,322 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:32:29,323 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:29,323 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:32:29,332 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:29,332 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2345 bytes\n",
            "2022-11-29 00:32:29,334 INFO reduce.MergeManagerImpl: Merged 1 segments, 2369 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:32:29,337 INFO reduce.MergeManagerImpl: Merging 1 files, 2373 bytes from disk\n",
            "2022-11-29 00:32:29,338 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:32:29,338 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:29,341 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2345 bytes\n",
            "2022-11-29 00:32:29,341 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:29,351 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer3.py]\n",
            "2022-11-29 00:32:29,354 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:32:29,355 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:32:29,385 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,385 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,389 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:29,456 INFO streaming.PipeMapRed: Records R/W=101/1\n",
            "2022-11-29 00:32:29,465 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:29,466 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:29,467 INFO mapred.Task: Task:attempt_local1843049360_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:29,468 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:29,468 INFO mapred.Task: Task attempt_local1843049360_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:32:29,470 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1843049360_0001_r_000000_0' to file:/Toplink_id/Toplink_idout_11\n",
            "2022-11-29 00:32:29,471 INFO mapred.LocalJobRunner: Records R/W=101/1 > reduce\n",
            "2022-11-29 00:32:29,471 INFO mapred.Task: Task 'attempt_local1843049360_0001_r_000000_0' done.\n",
            "2022-11-29 00:32:29,473 INFO mapred.Task: Final Counters for attempt_local1843049360_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=341047\n",
            "\t\tFILE: Number of bytes written=733485\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=66\n",
            "\t\tReduce shuffle bytes=2373\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=66\n",
            "\t\tSpilled Records=101\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1432\n",
            "2022-11-29 00:32:29,473 INFO mapred.LocalJobRunner: Finishing task: attempt_local1843049360_0001_r_000000_0\n",
            "2022-11-29 00:32:29,473 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:32:29,788 INFO mapreduce.Job: Job job_local1843049360_0001 running in uber mode : false\n",
            "2022-11-29 00:32:29,789 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:32:29,790 INFO mapreduce.Job: Job job_local1843049360_0001 completed successfully\n",
            "2022-11-29 00:32:29,798 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=677316\n",
            "\t\tFILE: Number of bytes written=1463165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2165\n",
            "\t\tMap output materialized bytes=2373\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=66\n",
            "\t\tReduce shuffle bytes=2373\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=66\n",
            "\t\tSpilled Records=202\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1432\n",
            "2022-11-29 00:32:29,798 INFO streaming.StreamJob: Output directory: /Toplink_id/Toplink_idout_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /Toplink_id/Toplink_idout_11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c24b84-7c94-4f96-e2bd-9882a9b1b07c",
        "id": "tMoKNPA0dSLf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:32 /Toplink_id/Toplink_idout_11/_SUCCESS\n",
            "-rw-r--r--   1 root root       1412 2022-11-29 00:32 /Toplink_id/Toplink_idout_11/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /Toplink_id/Toplink_idout_11/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f62642-362e-4c2d-af5e-4c9b5d047dd5",
        "id": "O_KnFySQdSLf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,t3_2pidx3,1\t\n",
            "AskReddit,t3_2qwm98,2\t\n",
            "AskReddit,t3_2qx5z4,1\t\n",
            "AskReddit,t3_2qx9vw,2\t\n",
            "AskReddit,t3_2qxfa5,1\t\n",
            "AskReddit,t3_2qxogl,1\t\n",
            "AskReddit,t3_2qxxsz,1\t\n",
            "AskReddit,t3_2qy2qk,2\t\n",
            "AskReddit,t3_2qy36l,1\t\n",
            "AskReddit,t3_2qy8r4,6\t\n",
            "AskReddit,t3_2qyig3,1\t\n",
            "AskReddit,t3_2qykcw,2\t\n",
            "AskReddit,t3_2qykg1,1\t\n",
            "AskReddit,t3_2qykih,1\t\n",
            "AskReddit,t3_2qylfy,1\t\n",
            "AskReddit,t3_2qynwn,1\t\n",
            "AskReddit,t3_2qyobl,1\t\n",
            "AskReddit,t3_2qyqs9,1\t\n",
            "AskReddit,t3_2qyrgt,1\t\n",
            "AskReddit,t3_2qyrje,1\t\n",
            "CFB,t3_2qy42q,1\t\n",
            "CFB,t3_2qy94r,3\t\n",
            "CFB,t3_2qyppp,1\t\n",
            "Games,t3_2qwmyo,1\t\n",
            "Games,t3_2qxl90,2\t\n",
            "Games,t3_2qy45k,1\t\n",
            "funny,t3_2qx73h,1\t\n",
            "funny,t3_2qxc5y,1\t\n",
            "funny,t3_2qxkcx,1\t\n",
            "funny,t3_2qxubc,1\t\n",
            "funny,t3_2qy0ft,5\t\n",
            "funny,t3_2qy7at,1\t\n",
            "funny,t3_2qyoc4,1\t\n",
            "funny,t3_2qyqyr,1\t\n",
            "funny,t3_2qyrvt,1\t\n",
            "hockey,t3_2qxi9c,1\t\n",
            "hockey,t3_2qxjzp,1\t\n",
            "hockey,t3_2qy54g,1\t\n",
            "hockey,t3_2qyim8,1\t\n",
            "hockey,t3_2qyjqv,2\t\n",
            "hockey,t3_2qyktv,1\t\n",
            "hockey,t3_2qyp3l,1\t\n",
            "hockey,t3_2qypbg,1\t\n",
            "leagueoflegends,t3_2qwdix,1\t\n",
            "leagueoflegends,t3_2qxntu,2\t\n",
            "leagueoflegends,t3_2qxzjm,1\t\n",
            "leagueoflegends,t3_2qy50m,1\t\n",
            "leagueoflegends,t3_2qyq44,1\t\n",
            "news,t3_2qwzdl,1\t\n",
            "news,t3_2qx8kh,4\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /Toplink_id/Toplink_idout_11/part-00000 >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt'"
      ],
      "metadata": {
        "id": "BQa6ydP-dSLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top linked_iD 2"
      ],
      "metadata": {
        "id": "wdba9CnfdA0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /Toplink_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkU-2xFCfjIT",
        "outputId": "4eb36071-f55d-40d5-ab65-1c98348bec4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/Toplink_id': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt' /Toplink_id"
      ],
      "metadata": {
        "id": "_y5ZMPpqfjIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper4.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "TopSubreddit = ['AskReddit', 'pcmasterrace', 'funny', 'pics', 'hockey', 'nfl', 'Games', 'news', 'leagueoflegends', 'CFB']\n",
        "# reading entire line from STDIN (standard input)\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.replace('\\n','')\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[0]\n",
        "        link_id = line [1]\n",
        "        count = line[2]\n",
        "        print(str(subreddit)+','+str(link_id)+','+str(count))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d824066-3c63-472b-c7b6-da6a0b2878c8",
        "id": "D5KLdEbUfjIV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer4.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, link_id, count = line.split(',')\n",
        "    if subreddit in subreddit_count:\n",
        "        subreddit_count[subreddit].append([link_id,int(count)])\n",
        "    else:\n",
        "        subreddit_count[subreddit] = []\n",
        "        subreddit_count[subreddit].append([link_id,int(count)])\n",
        "\n",
        "highest = 0\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "  for i in subreddit_count[subreddit]:\n",
        "   if  highest < i[1]:\n",
        "     highest = i[1]\n",
        "     link_id = i[0]\n",
        "  print(subreddit+ \",\" + link_id + \",\" + str(highest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b271a8-9f06-4775-fe87-8f63bd95fc2f",
        "id": "8VyvOkwmfjIW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt' | python mapper4.py | sort -k1,1 | python reducer4.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b60b31-fd8f-4826-e20f-0b4a21789087",
        "id": "VLPsOmYPfjIW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,t3_2qy8r4,6\n",
            "CFB,t3_2qy8r4,6\n",
            "funny,t3_2qy8r4,6\n",
            "Games,t3_2qy8r4,6\n",
            "hockey,t3_2qy8r4,6\n",
            "leagueoflegends,t3_2qy8r4,6\n",
            "news,t3_2qy8r4,6\n",
            "nfl,t3_2qy8r4,6\n",
            "pcmasterrace,t3_2qy3j3,9\n",
            "pics,t3_2qy3j3,9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper4.py /content/reducer4.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "Y0xP3HILfjIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt' \\\n",
        "  -output /Toplink_id/Toplink_idout_22 \\\n",
        "  -mapper \"python /content/mapper4.py\" \\\n",
        "  -reducer \"python /content/reducer4.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e17c731-82b5-434c-a903-d79d3531f0a4",
        "id": "oSqX4ERWfjIX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:32:40,201 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:32:40,321 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:32:40,321 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:32:40,350 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:40,559 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:32:40,582 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:32:40,770 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1811966102_0001\n",
            "2022-11-29 00:32:40,770 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:32:40,948 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:32:40,950 INFO mapreduce.Job: Running job: job_local1811966102_0001\n",
            "2022-11-29 00:32:40,957 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:32:40,959 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:32:40,963 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:40,963 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:41,011 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:32:41,016 INFO mapred.LocalJobRunner: Starting task: attempt_local1811966102_0001_m_000000_0\n",
            "2022-11-29 00:32:41,046 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:41,046 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:41,076 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:41,086 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt:0+2824\n",
            "2022-11-29 00:32:41,103 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:32:41,184 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:32:41,184 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:32:41,184 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:32:41,184 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:32:41,184 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:32:41,188 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:32:41,200 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper4.py]\n",
            "2022-11-29 00:32:41,206 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:32:41,207 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:32:41,207 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:32:41,207 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:32:41,208 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:32:41,208 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:32:41,208 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:32:41,208 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:32:41,208 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:32:41,209 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:32:41,209 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:32:41,209 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:32:41,233 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,233 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,235 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,348 INFO streaming.PipeMapRed: Records R/W=132/1\n",
            "2022-11-29 00:32:41,356 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:41,357 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:41,360 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:32:41,360 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:32:41,361 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:32:41,361 INFO mapred.MapTask: bufstart = 0; bufend = 2824; bufvoid = 104857600\n",
            "2022-11-29 00:32:41,361 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213872(104855488); length = 525/6553600\n",
            "2022-11-29 00:32:41,402 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:32:41,418 INFO mapred.Task: Task:attempt_local1811966102_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:41,420 INFO mapred.LocalJobRunner: Records R/W=132/1\n",
            "2022-11-29 00:32:41,420 INFO mapred.Task: Task 'attempt_local1811966102_0001_m_000000_0' done.\n",
            "2022-11-29 00:32:41,433 INFO mapred.Task: Final Counters for attempt_local1811966102_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=179578\n",
            "\t\tFILE: Number of bytes written=730496\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=132\n",
            "\t\tMap output records=132\n",
            "\t\tMap output bytes=2824\n",
            "\t\tMap output materialized bytes=3094\n",
            "\t\tInput split bytes=165\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=132\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2824\n",
            "2022-11-29 00:32:41,433 INFO mapred.LocalJobRunner: Finishing task: attempt_local1811966102_0001_m_000000_0\n",
            "2022-11-29 00:32:41,434 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:32:41,437 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:32:41,438 INFO mapred.LocalJobRunner: Starting task: attempt_local1811966102_0001_r_000000_0\n",
            "2022-11-29 00:32:41,447 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:32:41,447 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:32:41,448 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:32:41,452 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@23b37872\n",
            "2022-11-29 00:32:41,454 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:32:41,476 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:32:41,485 INFO reduce.EventFetcher: attempt_local1811966102_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:32:41,514 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1811966102_0001_m_000000_0 decomp: 3090 len: 3094 to MEMORY\n",
            "2022-11-29 00:32:41,518 INFO reduce.InMemoryMapOutput: Read 3090 bytes from map-output for attempt_local1811966102_0001_m_000000_0\n",
            "2022-11-29 00:32:41,520 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3090, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3090\n",
            "2022-11-29 00:32:41,521 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:32:41,522 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:41,523 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:32:41,531 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:41,532 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3066 bytes\n",
            "2022-11-29 00:32:41,534 INFO reduce.MergeManagerImpl: Merged 1 segments, 3090 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:32:41,535 INFO reduce.MergeManagerImpl: Merging 1 files, 3094 bytes from disk\n",
            "2022-11-29 00:32:41,536 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:32:41,536 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:32:41,536 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3066 bytes\n",
            "2022-11-29 00:32:41,537 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:41,546 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer4.py]\n",
            "2022-11-29 00:32:41,551 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:32:41,553 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:32:41,606 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,606 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,608 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:32:41,701 INFO streaming.PipeMapRed: Records R/W=132/1\n",
            "2022-11-29 00:32:41,704 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:32:41,705 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:32:41,706 INFO mapred.Task: Task:attempt_local1811966102_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:32:41,707 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:32:41,707 INFO mapred.Task: Task attempt_local1811966102_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:32:41,709 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1811966102_0001_r_000000_0' to file:/Toplink_id/Toplink_idout_22\n",
            "2022-11-29 00:32:41,710 INFO mapred.LocalJobRunner: Records R/W=132/1 > reduce\n",
            "2022-11-29 00:32:41,710 INFO mapred.Task: Task 'attempt_local1811966102_0001_r_000000_0' done.\n",
            "2022-11-29 00:32:41,711 INFO mapred.Task: Final Counters for attempt_local1811966102_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=185798\n",
            "\t\tFILE: Number of bytes written=733808\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=66\n",
            "\t\tReduce shuffle bytes=3094\n",
            "\t\tReduce input records=132\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=132\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=218\n",
            "2022-11-29 00:32:41,711 INFO mapred.LocalJobRunner: Finishing task: attempt_local1811966102_0001_r_000000_0\n",
            "2022-11-29 00:32:41,711 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:32:41,955 INFO mapreduce.Job: Job job_local1811966102_0001 running in uber mode : false\n",
            "2022-11-29 00:32:41,956 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:32:41,958 INFO mapreduce.Job: Job job_local1811966102_0001 completed successfully\n",
            "2022-11-29 00:32:41,966 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=365376\n",
            "\t\tFILE: Number of bytes written=1464304\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=132\n",
            "\t\tMap output records=132\n",
            "\t\tMap output bytes=2824\n",
            "\t\tMap output materialized bytes=3094\n",
            "\t\tInput split bytes=165\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=66\n",
            "\t\tReduce shuffle bytes=3094\n",
            "\t\tReduce input records=132\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=264\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2824\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=218\n",
            "2022-11-29 00:32:41,966 INFO streaming.StreamJob: Output directory: /Toplink_id/Toplink_idout_22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /Toplink_id/Toplink_idout_22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e34593-04fd-45df-ea75-2b81abc61a91",
        "id": "00SpSf59fjIX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:32 /Toplink_id/Toplink_idout_22/_SUCCESS\n",
            "-rw-r--r--   1 root root        206 2022-11-29 00:32 /Toplink_id/Toplink_idout_22/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /Toplink_id/Toplink_idout_22/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ac5907-2982-4681-94ce-6155a80648b8",
        "id": "d7P3DKEsfjIX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,t3_2qy8r4,6\t\n",
            "CFB,t3_2qy8r4,6\t\n",
            "Games,t3_2qy8r4,6\t\n",
            "funny,t3_2qy8r4,6\t\n",
            "hockey,t3_2qy8r4,6\t\n",
            "leagueoflegends,t3_2qy8r4,6\t\n",
            "news,t3_2qy8r4,6\t\n",
            "nfl,t3_2qy8r4,6\t\n",
            "pcmasterrace,t3_2qy3j3,9\t\n",
            "pics,t3_2qy3j3,9\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /Toplink_id/Toplink_idout_22/part-00000 >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Toplink_is_out1.txt'"
      ],
      "metadata": {
        "id": "Gi0ihUjpfjIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top username 1"
      ],
      "metadata": {
        "id": "u5k_FJVwdGGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /Topusername"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ8xS-PJfn9V",
        "outputId": "bf01dfd8-701c-4a3b-fcf1-1e3e5ee37466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/Topusername': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' /Topusername"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8fjbAFsfn9V",
        "outputId": "9e5cc890-8441-466d-f3d9-6372da05065e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `/Topusername/out.csv': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper5.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "TopSubreddit = ['AskReddit', 'pcmasterrace', 'funny', 'pics', 'hockey', 'nfl', 'Games', 'news', 'leagueoflegends', 'CFB']\n",
        "# reading entire line from STDIN (standard input)\n",
        "i=0\n",
        "for line in sys.stdin:\n",
        "    if i == 0:\n",
        "        i += 1\n",
        "        continue\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[13]\n",
        "        author = line [8]\n",
        "        count = 1\n",
        "        if subreddit in TopSubreddit:\n",
        "           print(str(subreddit)+','+str(author)+','+str(count))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643bbc4c-a9d6-4310-8636-b3824a40f062",
        "id": "FnNPPDthfn9W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer5.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, author, count = line.split(',')\n",
        "    if (subreddit,author) in subreddit_count:\n",
        "        subreddit_count[(subreddit,author)].append(int(count))\n",
        "    else:\n",
        "        subreddit_count[(subreddit,author)] = []\n",
        "        subreddit_count[(subreddit,author)].append(int(count))\n",
        "\n",
        "\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "    total_count = sum(subreddit_count[subreddit])\n",
        "    print(str(subreddit[0]) + \",\" +str(subreddit[1]) + \",\" + str(total_count))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9122910e-9906-487f-c20b-61eef4cecf32",
        "id": "XaE-p_3Ffn9X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' | python mapper5.py | sort -k1,1 | python reducer5.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447fc981-64ec-452f-ec38-19b3852a0bb0",
        "id": "BzNlxGNJfn9X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,A_french_chinese_man,1\n",
            "AskReddit,Angam23,1\n",
            "AskReddit,Beezzy,1\n",
            "AskReddit,Bionaknight,1\n",
            "AskReddit,Clippythe_Paperclip,1\n",
            "AskReddit,dirtymoney,1\n",
            "AskReddit,fisticuffs32,1\n",
            "AskReddit,hippiedude23615,1\n",
            "AskReddit,ibbity,1\n",
            "AskReddit,jakenash,1\n",
            "AskReddit,joshl7,1\n",
            "AskReddit,joshysea,1\n",
            "AskReddit,Leviathan666,1\n",
            "AskReddit,marcocholo,1\n",
            "AskReddit,moomanmonk,1\n",
            "AskReddit,Movepeck,1\n",
            "AskReddit,mwagner26,1\n",
            "AskReddit,mynewaccount5,1\n",
            "AskReddit,obpwrx,1\n",
            "AskReddit,pappajay2001,1\n",
            "AskReddit,pizzaleftovers,1\n",
            "AskReddit,Stevie_Rave_On,1\n",
            "AskReddit,thedude018,1\n",
            "AskReddit,theflealee,1\n",
            "AskReddit,TheRealPeteWheeler,1\n",
            "AskReddit,TomHanksDied,1\n",
            "AskReddit,TrainsILike,1\n",
            "AskReddit,Versimilitudinous,1\n",
            "AskReddit,whatthefuckguys,1\n",
            "CFB,47Ronin,1\n",
            "CFB,CaptaiinCrunch,1\n",
            "CFB,recoverybelow,1\n",
            "CFB,vanquish421,1\n",
            "CFB,zenmasterbrodie,1\n",
            "funny,BASSH34D,1\n",
            "funny,deephaven,1\n",
            "funny,devpsaux,1\n",
            "funny,Fusionism,1\n",
            "funny,IlllllI,1\n",
            "funny,LithePanther,1\n",
            "funny,LOHare,1\n",
            "funny,masone28,1\n",
            "funny,Meltingteeth,1\n",
            "funny,rundmc963,1\n",
            "funny,splice_of_life,1\n",
            "funny,superhole,1\n",
            "funny,thecabbler,1\n",
            "Games,Crodface,1\n",
            "Games,ItsMeCaptainMurphy,1\n",
            "Games,Jmrwacko,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper5.py /content/reducer5.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "ML5I5antfn9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv' \\\n",
        "  -output /Toplink_id/Topusernameout_11 \\\n",
        "  -mapper \"python /content/mapper5.py\" \\\n",
        "  -reducer \"python /content/reducer5.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e78168-5fdd-4452-ee10-5eec660d73dd",
        "id": "UmWB9c0Efn9Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:36:01,610 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:36:01,715 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:36:01,715 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:36:01,733 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:36:01,929 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:36:01,953 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:36:02,171 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1055885085_0001\n",
            "2022-11-29 00:36:02,171 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:36:02,354 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:36:02,356 INFO mapreduce.Job: Running job: job_local1055885085_0001\n",
            "2022-11-29 00:36:02,364 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:36:02,366 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:36:02,370 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:02,371 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:02,411 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:36:02,415 INFO mapred.LocalJobRunner: Starting task: attempt_local1055885085_0001_m_000000_0\n",
            "2022-11-29 00:36:02,445 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:02,445 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:02,486 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:36:02,496 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/out.csv:0+159546\n",
            "2022-11-29 00:36:02,513 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:36:02,591 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:36:02,591 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:36:02,591 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:36:02,591 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:36:02,591 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:36:02,595 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:36:02,607 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper5.py]\n",
            "2022-11-29 00:36:02,617 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:36:02,617 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:36:02,618 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:36:02,618 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:36:02,619 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:36:02,619 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:36:02,620 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:36:02,620 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:36:02,621 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:36:02,622 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:36:02,622 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:36:02,623 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:36:02,660 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:02,661 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:02,665 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:02,750 INFO streaming.PipeMapRed: Records R/W=455/1\n",
            "2022-11-29 00:36:02,764 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:36:02,765 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:36:02,768 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:36:02,768 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:36:02,768 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:36:02,768 INFO mapred.MapTask: bufstart = 0; bufend = 2313; bufvoid = 104857600\n",
            "2022-11-29 00:36:02,768 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213996(104855984); length = 401/6553600\n",
            "2022-11-29 00:36:02,783 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:36:02,802 INFO mapred.Task: Task:attempt_local1055885085_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:36:02,805 INFO mapred.LocalJobRunner: Records R/W=455/1\n",
            "2022-11-29 00:36:02,805 INFO mapred.Task: Task 'attempt_local1055885085_0001_m_000000_0' done.\n",
            "2022-11-29 00:36:02,817 INFO mapred.Task: Final Counters for attempt_local1055885085_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=336269\n",
            "\t\tFILE: Number of bytes written=729830\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2313\n",
            "\t\tMap output materialized bytes=2521\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=101\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "2022-11-29 00:36:02,817 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055885085_0001_m_000000_0\n",
            "2022-11-29 00:36:02,818 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:36:02,822 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:36:02,822 INFO mapred.LocalJobRunner: Starting task: attempt_local1055885085_0001_r_000000_0\n",
            "2022-11-29 00:36:02,830 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:02,830 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:02,831 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:36:02,834 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49f600e4\n",
            "2022-11-29 00:36:02,836 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:36:02,862 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:36:02,878 INFO reduce.EventFetcher: attempt_local1055885085_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:36:02,913 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1055885085_0001_m_000000_0 decomp: 2517 len: 2521 to MEMORY\n",
            "2022-11-29 00:36:02,916 INFO reduce.InMemoryMapOutput: Read 2517 bytes from map-output for attempt_local1055885085_0001_m_000000_0\n",
            "2022-11-29 00:36:02,917 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2517, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2517\n",
            "2022-11-29 00:36:02,919 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:36:02,920 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:02,920 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:36:02,927 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:36:02,928 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2482 bytes\n",
            "2022-11-29 00:36:02,930 INFO reduce.MergeManagerImpl: Merged 1 segments, 2517 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:36:02,931 INFO reduce.MergeManagerImpl: Merging 1 files, 2521 bytes from disk\n",
            "2022-11-29 00:36:02,932 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:36:02,932 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:36:02,933 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2482 bytes\n",
            "2022-11-29 00:36:02,933 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:02,943 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer5.py]\n",
            "2022-11-29 00:36:02,947 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:36:02,949 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:36:02,984 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:02,984 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:02,987 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:03,062 INFO streaming.PipeMapRed: Records R/W=101/1\n",
            "2022-11-29 00:36:03,070 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:36:03,071 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:36:03,071 INFO mapred.Task: Task:attempt_local1055885085_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:36:03,073 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:03,075 INFO mapred.Task: Task attempt_local1055885085_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:36:03,079 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055885085_0001_r_000000_0' to file:/Toplink_id/Topusernameout_11\n",
            "2022-11-29 00:36:03,080 INFO mapred.LocalJobRunner: Records R/W=101/1 > reduce\n",
            "2022-11-29 00:36:03,080 INFO mapred.Task: Task 'attempt_local1055885085_0001_r_000000_0' done.\n",
            "2022-11-29 00:36:03,080 INFO mapred.Task: Final Counters for attempt_local1055885085_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=341343\n",
            "\t\tFILE: Number of bytes written=734692\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=101\n",
            "\t\tReduce shuffle bytes=2521\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=101\n",
            "\t\tSpilled Records=101\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=2341\n",
            "2022-11-29 00:36:03,081 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055885085_0001_r_000000_0\n",
            "2022-11-29 00:36:03,081 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:36:03,363 INFO mapreduce.Job: Job job_local1055885085_0001 running in uber mode : false\n",
            "2022-11-29 00:36:03,364 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:36:03,365 INFO mapreduce.Job: Job job_local1055885085_0001 completed successfully\n",
            "2022-11-29 00:36:03,373 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=677612\n",
            "\t\tFILE: Number of bytes written=1464522\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=455\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2313\n",
            "\t\tMap output materialized bytes=2521\n",
            "\t\tInput split bytes=133\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=101\n",
            "\t\tReduce shuffle bytes=2521\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=101\n",
            "\t\tSpilled Records=202\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=159546\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=2341\n",
            "2022-11-29 00:36:03,380 INFO streaming.StreamJob: Output directory: /Toplink_id/Topusernameout_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /Toplink_id/Topusernameout_11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9768a4d-61d1-478d-a6e6-52f4418f33d0",
        "id": "BznmcUbJfn9Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:36 /Toplink_id/Topusernameout_11/_SUCCESS\n",
            "-rw-r--r--   1 root root       2313 2022-11-29 00:36 /Toplink_id/Topusernameout_11/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /Toplink_id/Topusernameout_11/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260d7d9d-97b3-4260-dcfc-f1d169c59b05",
        "id": "7uOuzBVNfn9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,A_french_chinese_man,1\t\n",
            "AskReddit,Angam23,1\t\n",
            "AskReddit,Beezzy,1\t\n",
            "AskReddit,Bionaknight,1\t\n",
            "AskReddit,Clippythe_Paperclip,1\t\n",
            "AskReddit,Leviathan666,1\t\n",
            "AskReddit,Movepeck,1\t\n",
            "AskReddit,Stevie_Rave_On,1\t\n",
            "AskReddit,TheRealPeteWheeler,1\t\n",
            "AskReddit,TomHanksDied,1\t\n",
            "AskReddit,TrainsILike,1\t\n",
            "AskReddit,Versimilitudinous,1\t\n",
            "AskReddit,dirtymoney,1\t\n",
            "AskReddit,fisticuffs32,1\t\n",
            "AskReddit,hippiedude23615,1\t\n",
            "AskReddit,ibbity,1\t\n",
            "AskReddit,jakenash,1\t\n",
            "AskReddit,joshl7,1\t\n",
            "AskReddit,joshysea,1\t\n",
            "AskReddit,marcocholo,1\t\n",
            "AskReddit,moomanmonk,1\t\n",
            "AskReddit,mwagner26,1\t\n",
            "AskReddit,mynewaccount5,1\t\n",
            "AskReddit,obpwrx,1\t\n",
            "AskReddit,pappajay2001,1\t\n",
            "AskReddit,pizzaleftovers,1\t\n",
            "AskReddit,thedude018,1\t\n",
            "AskReddit,theflealee,1\t\n",
            "AskReddit,whatthefuckguys,1\t\n",
            "CFB,47Ronin,1\t\n",
            "CFB,CaptaiinCrunch,1\t\n",
            "CFB,recoverybelow,1\t\n",
            "CFB,vanquish421,1\t\n",
            "CFB,zenmasterbrodie,1\t\n",
            "Games,Crodface,1\t\n",
            "Games,ItsMeCaptainMurphy,1\t\n",
            "Games,Jmrwacko,1\t\n",
            "Games,Kayjin23,1\t\n",
            "funny,BASSH34D,1\t\n",
            "funny,Fusionism,1\t\n",
            "funny,IlllllI,1\t\n",
            "funny,LOHare,1\t\n",
            "funny,LithePanther,1\t\n",
            "funny,Meltingteeth,1\t\n",
            "funny,deephaven,1\t\n",
            "funny,devpsaux,1\t\n",
            "funny,masone28,1\t\n",
            "funny,rundmc963,1\t\n",
            "funny,splice_of_life,1\t\n",
            "funny,superhole,1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /Toplink_id/Topusernameout_11/part-00000 >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusername1.txt'"
      ],
      "metadata": {
        "id": "CrGiBG-1fn9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top username 2"
      ],
      "metadata": {
        "id": "j-czITY_dJli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /Topusername"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0NVY3jRfsw1",
        "outputId": "d83e87ff-3514-4528-f084-1a12e5b110a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/Topusername': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusername1.txt' /Topusername"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LlzjSizfsw1",
        "outputId": "90c611b7-381b-4d56-c75b-167332618d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `/Topusername/Topusername1.txt': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper6.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "# reading entire line from STDIN (standard input)\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.replace('\\n','')\n",
        "    line = line.split(\",\")\n",
        "    if len(line) >=1:\n",
        "        subreddit = line[0]\n",
        "        author = line [1]\n",
        "        count = line[2]\n",
        "        print (str(subreddit)+','+str(author)+','+str(count))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ce5574-c886-481a-9a47-7f2bcfab9f53",
        "id": "0qucp_Btfsw1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper6.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer6.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "subreddit_count = {}\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    subreddit, link_id, count = line.split(',')\n",
        "    if subreddit in subreddit_count:\n",
        "        subreddit_count[subreddit].append([link_id,int(count)])\n",
        "    else:\n",
        "        subreddit_count[subreddit] = []\n",
        "        subreddit_count[subreddit].append([link_id,int(count)])\n",
        "\n",
        "highest = 0\n",
        "#Reducer\n",
        "for subreddit in subreddit_count.keys():\n",
        "  for i in subreddit_count[subreddit]:\n",
        "   if  highest < i[1]:\n",
        "     highest = i[1]\n",
        "     link_id = i[0]\n",
        "  print(subreddit+ \",\" + link_id + \",\" + str(highest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cad58c6-82a7-4dda-b72c-4d4e5b4cb72f",
        "id": "uKIxiie7fsw2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer6.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusername1.txt' | python mapper6.py | sort -k1,1 | python reducer6.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae699a7-1ac6-4171-9c27-077b96d5c438",
        "id": "oFuVJ1aQfsw2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,A_french_chinese_man,1\n",
            "CFB,A_french_chinese_man,1\n",
            "funny,A_french_chinese_man,1\n",
            "Games,A_french_chinese_man,1\n",
            "hockey,A_french_chinese_man,1\n",
            "leagueoflegends,A_french_chinese_man,1\n",
            "news,A_french_chinese_man,1\n",
            "nfl,A_french_chinese_man,1\n",
            "pcmasterrace,A_french_chinese_man,1\n",
            "pics,A_french_chinese_man,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper6.py /content/reducer6.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "pUvCHBMhfsw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusername1.txt' \\\n",
        "  -output /Toplink_id/Topusernameout_2 \\\n",
        "  -mapper \"python /content/mapper6.py\" \\\n",
        "  -reducer \"python /content/reducer6.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6561581e-d531-43c0-8e1b-c4abae7aa97a",
        "id": "Bl6vIvLGfsw3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-29 00:36:55,437 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-11-29 00:36:55,537 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-11-29 00:36:55,537 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-11-29 00:36:55,554 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:36:55,775 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-11-29 00:36:55,806 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-11-29 00:36:56,007 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1150982518_0001\n",
            "2022-11-29 00:36:56,007 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-11-29 00:36:56,200 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-11-29 00:36:56,203 INFO mapreduce.Job: Running job: job_local1150982518_0001\n",
            "2022-11-29 00:36:56,208 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-11-29 00:36:56,210 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-11-29 00:36:56,216 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:56,216 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:56,263 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-11-29 00:36:56,267 INFO mapred.LocalJobRunner: Starting task: attempt_local1150982518_0001_m_000000_0\n",
            "2022-11-29 00:36:56,296 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:56,296 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:56,331 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:36:56,344 INFO mapred.MapTask: Processing split: file:/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusername1.txt:0+2313\n",
            "2022-11-29 00:36:56,362 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-11-29 00:36:56,445 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-11-29 00:36:56,445 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-11-29 00:36:56,445 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-11-29 00:36:56,445 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-11-29 00:36:56,445 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-11-29 00:36:56,449 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-11-29 00:36:56,461 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper6.py]\n",
            "2022-11-29 00:36:56,470 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-11-29 00:36:56,470 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-11-29 00:36:56,472 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-11-29 00:36:56,472 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-11-29 00:36:56,473 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-11-29 00:36:56,473 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-11-29 00:36:56,474 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-11-29 00:36:56,476 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-11-29 00:36:56,476 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-11-29 00:36:56,477 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-11-29 00:36:56,477 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-11-29 00:36:56,477 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-11-29 00:36:56,515 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,515 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,517 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,603 INFO streaming.PipeMapRed: Records R/W=101/1\n",
            "2022-11-29 00:36:56,606 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:36:56,607 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:36:56,611 INFO mapred.LocalJobRunner: \n",
            "2022-11-29 00:36:56,611 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-11-29 00:36:56,611 INFO mapred.MapTask: Spilling map output\n",
            "2022-11-29 00:36:56,611 INFO mapred.MapTask: bufstart = 0; bufend = 2313; bufvoid = 104857600\n",
            "2022-11-29 00:36:56,611 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213996(104855984); length = 401/6553600\n",
            "2022-11-29 00:36:56,623 INFO mapred.MapTask: Finished spill 0\n",
            "2022-11-29 00:36:56,653 INFO mapred.Task: Task:attempt_local1150982518_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:36:56,656 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
            "2022-11-29 00:36:56,656 INFO mapred.Task: Task 'attempt_local1150982518_0001_m_000000_0' done.\n",
            "2022-11-29 00:36:56,666 INFO mapred.Task: Final Counters for attempt_local1150982518_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=179064\n",
            "\t\tFILE: Number of bytes written=729914\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=101\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2313\n",
            "\t\tMap output materialized bytes=2521\n",
            "\t\tInput split bytes=162\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=101\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2313\n",
            "2022-11-29 00:36:56,666 INFO mapred.LocalJobRunner: Finishing task: attempt_local1150982518_0001_m_000000_0\n",
            "2022-11-29 00:36:56,667 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-11-29 00:36:56,670 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-11-29 00:36:56,671 INFO mapred.LocalJobRunner: Starting task: attempt_local1150982518_0001_r_000000_0\n",
            "2022-11-29 00:36:56,680 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-11-29 00:36:56,680 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-11-29 00:36:56,680 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-11-29 00:36:56,684 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@23b37872\n",
            "2022-11-29 00:36:56,686 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-11-29 00:36:56,708 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-11-29 00:36:56,723 INFO reduce.EventFetcher: attempt_local1150982518_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-11-29 00:36:56,758 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1150982518_0001_m_000000_0 decomp: 2517 len: 2521 to MEMORY\n",
            "2022-11-29 00:36:56,761 INFO reduce.InMemoryMapOutput: Read 2517 bytes from map-output for attempt_local1150982518_0001_m_000000_0\n",
            "2022-11-29 00:36:56,763 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2517, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2517\n",
            "2022-11-29 00:36:56,764 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-11-29 00:36:56,765 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:56,765 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-11-29 00:36:56,774 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:36:56,775 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2482 bytes\n",
            "2022-11-29 00:36:56,777 INFO reduce.MergeManagerImpl: Merged 1 segments, 2517 bytes to disk to satisfy reduce memory limit\n",
            "2022-11-29 00:36:56,778 INFO reduce.MergeManagerImpl: Merging 1 files, 2521 bytes from disk\n",
            "2022-11-29 00:36:56,779 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-11-29 00:36:56,779 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-11-29 00:36:56,780 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2482 bytes\n",
            "2022-11-29 00:36:56,780 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:56,791 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer6.py]\n",
            "2022-11-29 00:36:56,797 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-11-29 00:36:56,799 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-11-29 00:36:56,827 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,828 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,832 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-11-29 00:36:56,920 INFO streaming.PipeMapRed: Records R/W=101/1\n",
            "2022-11-29 00:36:56,924 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-11-29 00:36:56,925 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-11-29 00:36:56,925 INFO mapred.Task: Task:attempt_local1150982518_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-11-29 00:36:56,927 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-11-29 00:36:56,927 INFO mapred.Task: Task attempt_local1150982518_0001_r_000000_0 is allowed to commit now\n",
            "2022-11-29 00:36:56,929 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1150982518_0001_r_000000_0' to file:/Toplink_id/Topusernameout_2\n",
            "2022-11-29 00:36:56,933 INFO mapred.LocalJobRunner: Records R/W=101/1 > reduce\n",
            "2022-11-29 00:36:56,933 INFO mapred.Task: Task 'attempt_local1150982518_0001_r_000000_0' done.\n",
            "2022-11-29 00:36:56,933 INFO mapred.Task: Final Counters for attempt_local1150982518_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=184138\n",
            "\t\tFILE: Number of bytes written=732763\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=101\n",
            "\t\tReduce shuffle bytes=2521\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=101\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=328\n",
            "2022-11-29 00:36:56,934 INFO mapred.LocalJobRunner: Finishing task: attempt_local1150982518_0001_r_000000_0\n",
            "2022-11-29 00:36:56,934 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-11-29 00:36:57,214 INFO mapreduce.Job: Job job_local1150982518_0001 running in uber mode : false\n",
            "2022-11-29 00:36:57,215 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-11-29 00:36:57,216 INFO mapreduce.Job: Job job_local1150982518_0001 completed successfully\n",
            "2022-11-29 00:36:57,224 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=363202\n",
            "\t\tFILE: Number of bytes written=1462677\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=101\n",
            "\t\tMap output records=101\n",
            "\t\tMap output bytes=2313\n",
            "\t\tMap output materialized bytes=2521\n",
            "\t\tInput split bytes=162\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=101\n",
            "\t\tReduce shuffle bytes=2521\n",
            "\t\tReduce input records=101\n",
            "\t\tReduce output records=10\n",
            "\t\tSpilled Records=202\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2313\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=328\n",
            "2022-11-29 00:36:57,224 INFO streaming.StreamJob: Output directory: /Toplink_id/Topusernameout_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /Toplink_id/Topusernameout_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925b541d-7eb9-445b-eb84-8585169d9529",
        "id": "2ynoW3ZVfsw3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-11-29 00:36 /Toplink_id/Topusernameout_2/_SUCCESS\n",
            "-rw-r--r--   1 root root        316 2022-11-29 00:36 /Toplink_id/Topusernameout_2/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /Toplink_id/Topusernameout_2/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2676090a-be2c-436b-879b-6931b2a4aff1",
        "id": "_4j1PXB7fsw4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AskReddit,A_french_chinese_man,1\t\n",
            "CFB,A_french_chinese_man,1\t\n",
            "Games,A_french_chinese_man,1\t\n",
            "funny,A_french_chinese_man,1\t\n",
            "hockey,A_french_chinese_man,1\t\n",
            "leagueoflegends,A_french_chinese_man,1\t\n",
            "news,A_french_chinese_man,1\t\n",
            "nfl,A_french_chinese_man,1\t\n",
            "pcmasterrace,A_french_chinese_man,1\t\n",
            "pics,A_french_chinese_man,1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /Toplink_id/Topusernameout_2/part-00000 >> '/content/drive/MyDrive/Fall 2022_ submissions/Big Data Miniproject 1/With Hadoop/1st Req/Topusenrname_out2.txt'"
      ],
      "metadata": {
        "id": "TZ9MfvQ-fsw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}